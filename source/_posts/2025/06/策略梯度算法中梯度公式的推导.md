---
title: 策略梯度算法中梯度公式的推导
disableNunjucks: false
mathjax: true
id: 12633
date: 2025-06-09 10:04:20
categories:
  - [机器学习]
  - [学习笔记]
tags:
  - 强化学习
cover: https://blogfiles.oss.fyz666.xyz/webp/2491389d-9d02-4a1a-9bdb-0765162ed58f.webp
---

最近学习强化学习的策略梯度算法时，遇到其中策略梯度的计算部分，一些推导的细节在我所学习的视频中被一笔带过了，而这些推导过程本该十分重要，故在本文中简单整理一下。

首先表述几个基本的符号：

- $$S$$ 是全体状态的集合，$$A$$ 是全体动作的集合。
- $$\pi_\theta(*|s_t)$$ 表示以 $$\theta$$ 为参数、在 $$t$$ 时间步的状态 $$s_t$$ 下的动作策略，是一个概率分布。$$\theta$$ 即是本算法优化的参数。
- $$R_t$$ 表示时间步 $$t$$ 的即时奖励。
- $$U_t=\sum_{i=t}^n\gamma^{i-t}R_i$$ 表示从时间步 $$t$$ 开始的累计回报。
- $$Q(s_t,a_t)=\mathbb{E}[U_t|s=s_t,a=a_t]$$ 表示在状态 $$s_t$$ 时采取动作 $$a_t$$ 时能获得的累计回报 $$U_t$$ 的期望值。
- $$V_{\pi_\theta}(s_t)=\mathbb{E}_{a_t\sim\pi_\theta(*|s_t)}[Q(s_t,a_t)]$$ 表示在状态 $$s_t$$ 下，使用动作策略 $$\pi_\theta$$ 时，$$Q$$ 函数的期望值。
- $$J(\theta)=\mathbb{E}_{s\in S}[V_{\pi_\theta}(s)]$$ 表示采取 以$$\theta$$ 为参数的动作策略 $$\pi_\theta$$ 时，能获得的所有状态下的回报的期望值。

考虑到希望在策略 $$\pi_\theta$$ 下获得尽可能高的回报，我们的优化目标自然是：

$$\max_{\theta\in\Theta}J(\theta)$$

故需要计算 $$\nabla_\theta J(\theta)$$，计算此梯度时运用了一个称为「Log Derivative Trick」的技巧，下面推导一下这个梯度：

$$
\begin{aligned}
\nabla_\theta J(\theta)&=\nabla_\theta\mathbb{E}_{s\in S}[V_{\pi_\theta}(s)]\\

&=\nabla_\theta\mathbb{E}_{s\in S}\mathbb{E}_{a_t\sim\pi_\theta(*|s)}[Q(s,a_t)]\\

&=\mathbb{E}_{s\in S}\nabla_\theta\mathbb{E}_{a_t\sim\pi_\theta(*|s)}[Q(s,a_t)]\\

&=\mathbb{E}_{s\in S}\nabla_\theta\int_{a_t\in A}\pi_\theta(a_t|s)Q(s,a_t)\\

&=\mathbb{E}_{s\in S}\int_{a_t\in A}\nabla_\theta\pi_\theta(a_t|s)Q(s,a_t)

\end{aligned}
$$

上面积分式中，仅有 $$\pi_\theta(a_t|s)$$ 依赖于参数 $$\theta$$，故可以单独计算此项：

$$
\nabla_\theta\pi_\theta(a_t|s)
$$

但这里梯度的外面还套着一个积分，算起来十分麻烦，因此使用了一个非常基本的对数导数等式：

$$\frac{\partial(\log{f(x)})}{\partial x}=\frac{\partial(f(x))}{\partial x}\frac{1}{f(x)}$$


应用上式，有：

$$
\nabla_\theta\pi_\theta(a_t|s)=\nabla_\theta(\log{\pi_\theta(a_t|s)})\pi_\theta(a_t|s)
$$

这么做的目的是在积分里面凑出来了一个概率分布：$$\pi_\theta(*|s)$$，从而可以将前面的积分转化为一个新的期望：

$$
\int_{a_t\in A}\nabla_\theta(\log{\pi_\theta(a_t|s)})\pi_\theta(a_t|s)Q(s,a_t)=\mathbb{E}_{a_t\sim\pi_\theta(*|s)}[\nabla_\theta(\log{\pi_\theta(a_t|s)})Q(s,a_t)]
$$

则梯度可表示为：

$$\nabla_\theta J(\theta)=\mathbb{E}_{s\in S}\mathbb{E}_{a_t\sim\pi_\theta(*|s)}[\nabla_\theta(\log{\pi_\theta(a_t|s)})Q(s,a_t)]$$

这样，就顺利的把梯度运算符移到了嵌套期望的内部，接下来的一个常用技巧是使用蒙特卡洛采样算法去估计 $$Q(s,a_t)$$ 这一项的期望值。

$$
Q(s,a_t)\approx\sum_{i=t}^n\gamma^{i-t}R_i
$$

如此一来，计算 $$J(\theta)$$ 对 $$\theta$$ 的梯度值就有了从代码上实现的可行性。


假设在某个策略 $$\pi_\theta$$ 下收集了足够多的游戏数据：

$$\{(s_t,a_t,R_t)|t=0,\dots,T\}$$

我们只需计算：

$$J(\theta)\approx\sum_{t=0}^T(\sum_{i=t}^T\gamma^{i-t}R_i)\log{\pi_\theta(a_t|s_t)}$$


然后应用反向传播与梯度上升算法，即可优化 $$\theta$$。