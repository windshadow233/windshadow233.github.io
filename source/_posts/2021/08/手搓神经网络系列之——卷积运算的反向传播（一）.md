---
title: 手搓神经网络系列之——卷积运算的反向传播（一）
id: 6034
date: 2021-08-04 17:51:08
categories:
    - [手搓神经网络]
tags: ['卷积神经网络', '反向传播', '神经网络', '机器学习', '深度学习']
cover: https://blogfiles.oss.fyz666.xyz/png/a7d63dcf-617d-4f7c-a3a5-8c5f8fa1ef08.png
disableNunjucks: false
mathjax: true
---

前面写了整整三篇文章讨论了卷积运算的正向传播，本文将进入卷积运算的反向传播部分，将涉及到一些简单的数学公式推导（与其说是推导，不如说是瞪眼法+直接写结论），都是最简单的线性函数，不必裂开。

本系列全部代码见下面仓库：

{% link autograd-with-numpy,GitHub,https://github.com/windshadow233/autograd-with-numpy %}

如有算法或实现方式上的问题，请各位大佬轻喷+指正！

我们以下面简单的卷积过程为例，推导卷积运算的梯度传播式：

![](https://blogfiles.oss.fyz666.xyz/png/4f96a835-ab19-43bd-a07a-621b852bff3b.png)
我们将上面的卷积过程展开写出来，得到下面4个方程：


$$\left\lbrace\begin{aligned}z_{00}=x_{00}w_{00}+x_{01}w_{01}+x_{10}w_{10}+x_{11}w_{11}\\z_{01}=x_{01}w_{00}+x_{02}w_{01}+x_{11}w_{10}+x_{12}w_{11}\\z_{10}=x_{10}w_{00}+x_{11}w_{01}+x_{20}w_{10}+x_{21}w_{11}\\z_{11}=x_{11}w_{00}+x_{12}w_{01}+x_{21}w_{10}+x_{22}w_{11}\end{aligned}\right.$$


已知传至张量$Z$的梯度为$\delta_Z$，我们分别对数据张量$X$和卷积核张量$W$计算梯度。


## 对X的梯度


下面先计算对$X$的梯度$\delta_X$，通过简单的链式法则即可得到：


$$\left\lbrace\begin{aligned}&\delta_{x_{00}}=w_{00}\delta_{z_{00}}\\&\delta_{x_{01}}=w_{01}\delta_{z_{00}}+w_{00}\delta_{z_{01}}\\&\delta_{x_{02}}=w_{01}\delta_{z_{01}}\\&\delta_{x_{10}}=w_{10}\delta_{z_{00}}+w_{00}\delta_{z_{10}}\\&\delta_{x_{11}}=w_{11}\delta_{z_{00}}+w_{10}\delta{z_{01}}+w_{01}\delta{z_{10}}+w_{00}\delta_{z_{11}}\\&\delta_{x_{12}}=w_{11}\delta_{z_{01}}+w_{01}\delta_{z_{11}}\\&\delta_{x_{20}}=w_{10}\delta_{z_{10}}\\&\delta_{x_{21}}=w_{11}\delta_{z_{10}}+w_{10}\delta_{z_{11}}\\&\delta_{x_{22}}=w_{11}\delta_{z_{11}}\end{aligned}\right.$$


乍一看十分复杂，但事实上，通过瞪眼法我们可得出，这是以下卷积过程的展开式：



![](https://blogfiles.oss.fyz666.xyz/png/a4216952-f67d-4154-bf9b-1f701d81fc29.png)
其中，左侧是$Z$的梯度矩阵$\delta_Z$经过一圈padding之后的样子，中间的卷积核是原来的卷积核$W$，经过180度的旋转所得到的，回顾前一篇文章所讲到的，这种旋转相当于张量所有元素在内存上的顺序reverse了一下。


需要注意的是，若正向卷积时的步长大于1，那么在这里计算梯度$\delta_X$的时候，需要对梯度$\delta_Z$额外进行一次插入0的操作，我将这种操作称为dilate，各位可以自己去推导一下，看看究竟需要做什么操作。


我们可以将上面的梯度表达式简写为：


$$\delta_X=pad(dilate(\delta_Z)) \ast rotate180(W)$$


通过简单的推导，可以发现，pad的圈数，应该与正向卷积的步长是相关的，不过其实还有一个巧妙的算法，即通过这三者的形状进行反推，感兴趣的话可以去推导一下。




---

## 对W的梯度


接下来计算对卷积核$W$的梯度$\delta_W$，同样由前面的方程进行链式法则计算：


$$\left\lbrace\begin{aligned}&\delta_{w_{00}}=x_{00}\delta_{z_{00}}+x_{01}\delta_{z_{01}}+x_{10}\delta_{z_{10}}+x_{11}\delta_{z_{11}}\\&\delta_{w_{01}}=x_{01}\delta_{z_{00}}+x_{02}\delta_{z_{01}}+x_{11}\delta_{z_{10}}+x_{12}\delta_{z_{11}}\\&\delta_{w_{10}}=x_{10}\delta_{z_{00}}+x_{11}\delta_{z_{01}}+x_{20}\delta_{z_{10}}+x_{21}\delta_{z_{11}}\\&\delta_{w_{11}}=x_{11}\delta_{z_{00}}+x_{12}\delta_{z_{01}}+x_{21}\delta_{z_{10}}+x_{22}\delta_{z_{11}}\end{aligned}\right.$$


这一组公式的规律更加明显了，其相当于下面的卷积运算的展开式：



![](https://blogfiles.oss.fyz666.xyz/png/1de34fe0-4135-42f9-9b9c-d325170c4a07.png)
与前面计算$\delta_X$时类似，若正向卷积时的步长大于1，那么在这里计算梯度$\delta_W$的时候，同样需要对梯度$\delta_Z$进行dilate操作，不过这里不需要pad 0。


我们同样可以将上面的梯度表达式简写为：


$$\delta_W=X \ast dilate(\delta_Z)$$




---

以上，即是卷积运算的梯度传播公式，即便是高维情况下的卷积运算，也万变不离其宗。本文虽短，但思考为什么卷积的梯度传递会是这样的形式让我死了很多脑细胞。后面一篇文章，将进入卷积运算反向传播的代码实现部分！
