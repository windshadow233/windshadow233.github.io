本文介绍了DPO（Direct Preference Optimization）算法的原理与实现。DPO通过人类偏好对比数据直接优化模型，避免了奖励模型训练和强化学习阶段。其优化目标基于Bradley-Terry模型，通过最大化偏好数据的对数似然来调整模型生成行为。训练中使用了OpenLLMAI/comparison_data数据集，并通过对比DPO模型与SFT模型的输出，展示了DPO在处理诱导性问题时更强的拒绝能力，同时在正常问答中保持良好响应。