本文介绍了大语言模型预训练的基本原理，以GPT为例，解释了其基于Transformer架构的自回归建模方式，强调了预训练阶段通过预测下一个token任务让模型学习语言结构和规律的过程，并指出预训练是大语言模型能力构建的基础。