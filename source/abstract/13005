本文记录了作者在一块 NVIDIA 3090 显卡上部署 Qwen3-32B 模型的尝试，并通过 Cherry Studio 接入模型服务的全过程。Qwen3-32B 是一个混合推理模型，具备“思考模式”和“非思考模式”，能根据任务复杂度灵活响应。作者使用 `llama_server` 部署模型，并为适配 Cherry Studio，自定义了 Jinja 模板及代理脚本 `llm_proxy.py`，实现了请求转发、鉴权与参数传递功能。此外，作者还介绍了如何在 Cherry Studio 中配置模型服务、助手、知识库、网络搜索等模块，并推荐使用“函数”方式调用工具，以提高效率。最后，作者分享了 Cherry Studio 的划词助手等实用插件。整体展示了从模型部署到前端接入的完整实践。